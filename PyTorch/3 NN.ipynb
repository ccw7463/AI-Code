{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chang\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 데이터구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):    \n",
    "    # 객체생성시 한번실행\n",
    "    def __init__(self, data_path, transform=None, target_transform=None):\n",
    "        \n",
    "        data = pd.read_csv(data_path)\n",
    "        Y_data = data['label']\n",
    "        Y_data = np.array(Y_data)\n",
    "        X_data = data.drop(columns='label',axis=1)\n",
    "        X_data = np.array(X_data).reshape(-1,28,28,1).astype('float32')\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    # 데이터세트의 총 개수를 리턴\n",
    "    def __len__(self):\n",
    "        return len(self.Y_data)\n",
    "\n",
    "    # 인덱스를 기반으로 데이터세트로부터 샘플을 가져오는 함수 \n",
    "    def __getitem__(self, idx):            \n",
    "\n",
    "        # 데이터 지정\n",
    "        image = self.X_data[idx]\n",
    "        label = self.Y_data[idx]\n",
    "        \n",
    "        # 입력 데이터 전처리\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 출력 데이터 전처리\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_path = \"dataset/fashion-mnist_train.csv\"\n",
    "test_path = \"dataset/fashion-mnist_test.csv\"\n",
    "\n",
    "# transforms.ToTensor() : PIL이미지,Numpy 배열 --> Pytorch image tensor로 변환 (배열구조 변경 및 scale 변환됨)\n",
    "train_dataset = CustomDataset(train_path,\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "\n",
    "test_dataset = CustomDataset(test_path,\n",
    "                             transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"생성한 데이터세트 클래스\"를 상속받은 \"객체\"를 입력으로 받음\n",
    "# 데이터세트에서 데이터를 iterable 하게 가져옴\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 모델구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn: 신경망 구축을 위한 데이터 구조나 레이어 등의 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 정의한 모델 클래스를 gpu로 지정후 인스턴스 생성\n",
    "model = NN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 하이퍼파라미터 & LOSS & Optimizer 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader,model,loss_fn,optimizer):\n",
    "    \n",
    "    # 입력 데이터세트의 총 길이 \n",
    "    size = len(dataloader.dataset) # 60000\n",
    "\n",
    "    # 배치 총 개수 (60000/64 = 938) \n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # train_loop \n",
    "    for batch, (X,y) in enumerate(dataloader): \n",
    "\n",
    "        # 타입 & device 변환\n",
    "        X = X.type(torch.float32).to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # 이전 루프에서 각 파라미터들에 저장된 .grad를 초기화\n",
    "        loss.backward() # 각 파라미터들의 .grad값에 변화정도가 저장이 됨.\n",
    "        optimizer.step() # 최적화함수에 맞게, 각 파라미터 업데이트\n",
    "\n",
    "        # log\n",
    "        if batch % num_batches == 0:\n",
    "            loss = loss.item() # item()을 통해, 변수에서 값만 가져옴\n",
    "            current = batch*len(X)\n",
    "            print(f\"Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader,model,loss_fn):\n",
    "\n",
    "    # 입력 데이터세트의 총 길이\n",
    "    size = len(dataloader.dataset) # 10000\n",
    "    \n",
    "    # 배치 총 개수 (10000/64 = 157)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # 테스트 단계이므로 gradient 옵션 해제\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            \n",
    "            # 타입통일 & device 변환\n",
    "            X = X.type(torch.float32).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # 모델 예측\n",
    "            pred = model(X)\n",
    "\n",
    "            # Loss 계산\n",
    "            loss = loss_fn(pred,y)\n",
    "\n",
    "            # LOSS / CORRECT \n",
    "            test_loss += loss.item() # item()을 통해, 변수에서 값만 가져옴\n",
    "            correct_cnt = (pred.argmax(axis=1)==y).type(torch.float).sum().item()\n",
    "            correct += correct_cnt\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train loss: 0.136297  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.363977 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train loss: 0.153474  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.378791 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train loss: 0.104953  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.393948 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train loss: 0.089394  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.366364 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train loss: 0.049651  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.466652 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train loss: 0.157366  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.409780 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train loss: 0.089616  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.377564 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train loss: 0.091857  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.378367 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train loss: 0.052092  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.404902 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train loss: 0.029601  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.389171 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train loss: 0.031732  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.389684 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train loss: 0.025977  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.388960 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train loss: 0.017319  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.399875 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train loss: 0.060504  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.407510 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train loss: 0.082178  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.387667 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train loss: 0.036033  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.411837 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train loss: 0.045850  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.409497 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train loss: 0.035683  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.411039 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train loss: 0.021084  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.454603 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train loss: 0.039112  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.439608 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train loss: 0.028837  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.431290 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train loss: 0.076710  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.432465 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train loss: 0.055627  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.426086 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train loss: 0.013462  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.430345 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train loss: 0.010729  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.434013 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train loss: 0.021717  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.431676 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train loss: 0.041012  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.429757 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train loss: 0.089143  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.440589 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train loss: 0.027392  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.430541 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train loss: 0.024920  [    0/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.515323 \n",
      "\n",
      "학습완료\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 30\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"학습완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0004, -0.0104,  0.0025,  ..., -0.0168,  0.0240, -0.0120],\n",
       "                      [-0.0221, -0.0214, -0.0240,  ...,  0.0329, -0.0115, -0.0099],\n",
       "                      [ 0.0040,  0.0240, -0.0281,  ..., -0.0088,  0.0015, -0.0051],\n",
       "                      ...,\n",
       "                      [ 0.0048, -0.0296, -0.0335,  ...,  0.0081,  0.0206,  0.0207],\n",
       "                      [ 0.0229,  0.0224, -0.0221,  ...,  0.0150, -0.0139,  0.0048],\n",
       "                      [ 0.0312, -0.0229, -0.0215,  ..., -0.0234, -0.0288,  0.0225]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-1.8264e-02, -3.4706e-02,  3.4503e-02,  1.5933e-02,  3.3503e-02,\n",
       "                      -1.8268e-02, -1.7407e-02,  8.0160e-03, -3.2167e-02, -7.5695e-03,\n",
       "                       4.2844e-03,  2.8266e-02,  8.4228e-03,  3.5166e-02,  1.3006e-02,\n",
       "                       1.3063e-02,  8.7463e-04, -4.6216e-03,  4.1512e-03,  1.6867e-02,\n",
       "                       1.8873e-02,  2.8213e-02,  5.2224e-03, -1.6313e-02,  3.2417e-02,\n",
       "                       2.8424e-02,  3.3408e-02, -8.3909e-03, -2.0807e-02,  6.0852e-03,\n",
       "                       1.2184e-02,  3.2933e-02, -2.7579e-02, -3.2840e-02, -3.4792e-02,\n",
       "                      -3.5391e-02, -1.3749e-02, -2.4165e-02,  1.0152e-02,  3.4838e-02,\n",
       "                       1.7211e-02,  1.0226e-02,  2.2277e-02, -2.6061e-02,  1.8386e-02,\n",
       "                       1.4518e-02,  8.3806e-03, -7.0755e-03,  1.6986e-02, -3.1406e-02,\n",
       "                      -3.0293e-02,  1.2130e-02,  1.1672e-02, -2.0163e-02,  1.6228e-02,\n",
       "                      -2.5310e-02, -6.7486e-03, -1.7137e-02, -4.0906e-03,  7.3116e-03,\n",
       "                       3.5065e-02,  5.4443e-03, -2.5395e-02,  2.8752e-02,  1.7777e-02,\n",
       "                       3.0447e-03, -8.6387e-03, -2.9618e-02, -1.8224e-02,  3.0132e-02,\n",
       "                       2.3342e-02,  1.5591e-02,  6.4938e-06,  1.8531e-02, -2.2652e-02,\n",
       "                       2.9456e-02,  1.6676e-02, -1.2310e-03, -3.0660e-02,  2.0182e-02,\n",
       "                      -1.2056e-02, -1.7712e-02,  1.6864e-02, -2.6903e-02,  2.4528e-02,\n",
       "                      -2.7328e-02, -2.6010e-02,  1.3355e-02,  2.9730e-02, -2.3238e-02,\n",
       "                      -2.9083e-02,  3.3165e-02, -3.4136e-02,  2.5192e-02, -6.0590e-03,\n",
       "                       9.7885e-03, -3.3977e-03, -2.4033e-02,  1.4941e-02, -2.2912e-02,\n",
       "                       2.4438e-02, -1.7821e-02, -2.3447e-02,  2.7052e-02, -1.1420e-02,\n",
       "                       2.4505e-02,  9.0621e-03, -1.8613e-02,  7.7486e-03,  3.5214e-02,\n",
       "                      -2.8750e-02,  8.0259e-03, -8.1476e-03,  3.6430e-03,  1.4424e-02,\n",
       "                       2.8356e-03, -3.2209e-03,  2.2237e-02, -1.3110e-02, -2.8561e-04,\n",
       "                       8.5874e-03,  2.4856e-02,  2.6941e-02,  3.3170e-02, -6.4960e-05,\n",
       "                       5.1031e-03,  2.4082e-02,  2.1497e-02,  2.2673e-02,  1.2875e-02,\n",
       "                      -1.9475e-02, -4.4369e-03, -2.8012e-02,  3.5013e-02,  1.1852e-03,\n",
       "                       9.3379e-03,  4.0537e-03, -2.9770e-02,  5.5807e-03,  3.2263e-02,\n",
       "                       3.4101e-02, -9.4350e-03, -3.1675e-02,  3.1620e-02,  2.5823e-02,\n",
       "                       2.9131e-02,  2.5985e-02,  8.6906e-03,  1.1350e-02,  2.3845e-02,\n",
       "                      -2.5547e-02, -1.8174e-02,  8.7214e-03,  1.3935e-02, -1.8990e-02,\n",
       "                       5.1190e-03,  2.9863e-02,  2.8040e-02,  8.3844e-03, -5.8767e-03,\n",
       "                       3.4394e-02, -3.6894e-03,  2.9564e-02, -8.3141e-03,  2.9604e-02,\n",
       "                       5.1312e-03,  2.3374e-02, -2.1006e-02,  3.1625e-02, -2.5064e-02,\n",
       "                      -1.0233e-02,  1.3822e-02, -1.4152e-02, -1.1983e-02, -2.2648e-02,\n",
       "                       3.5283e-02, -2.9983e-03, -7.7506e-03, -2.3422e-02, -1.7472e-02,\n",
       "                       2.0802e-02, -2.7382e-02, -2.1315e-03,  7.4747e-03, -6.9584e-03,\n",
       "                      -8.0470e-03,  2.6449e-02,  2.2804e-02,  2.7890e-02,  8.2869e-03,\n",
       "                      -2.7400e-02,  2.6358e-02, -1.8084e-02, -1.0209e-03,  4.6785e-03,\n",
       "                       3.7892e-03,  1.5692e-02,  2.2431e-02,  6.7466e-03,  2.6151e-02,\n",
       "                       2.6038e-02, -1.7257e-02,  1.1676e-02, -3.0359e-02, -2.2094e-02,\n",
       "                      -1.5575e-02, -2.6110e-02, -3.0578e-02,  2.4348e-02, -9.6617e-04,\n",
       "                      -2.1295e-02,  1.6899e-02, -1.8418e-02, -6.5104e-03, -1.9383e-02,\n",
       "                      -2.2074e-02,  1.2188e-02,  8.9384e-03,  3.5124e-02, -1.6340e-02,\n",
       "                       1.1604e-03,  2.6406e-02,  2.5745e-02,  3.4762e-02,  8.8721e-03,\n",
       "                       9.5194e-03, -3.3670e-02, -3.3712e-03, -1.1846e-02, -2.6199e-02,\n",
       "                       1.8755e-02,  1.3975e-02,  1.1038e-03,  6.0394e-03,  2.8629e-02,\n",
       "                       8.2350e-03,  3.4539e-02, -2.4673e-02,  2.1575e-02, -9.8197e-03,\n",
       "                      -5.2862e-03, -2.7851e-02, -2.1356e-02,  1.8868e-02,  2.1253e-03,\n",
       "                      -2.8868e-04, -2.5324e-02,  2.5515e-02,  5.5983e-03,  4.8314e-03,\n",
       "                      -1.7439e-03, -2.7457e-03, -2.5192e-02,  2.1718e-02,  5.1233e-03,\n",
       "                      -3.2371e-02,  2.7675e-02,  2.4450e-02,  9.7602e-03, -6.4953e-04,\n",
       "                       1.4698e-02, -3.0274e-02, -2.9407e-03,  2.7983e-03,  3.3895e-02,\n",
       "                       7.7695e-03, -1.0375e-02,  3.2593e-02,  1.1359e-04, -3.0468e-02,\n",
       "                       3.2281e-03, -1.7680e-02,  2.6630e-02, -2.6646e-02,  2.6437e-02,\n",
       "                      -1.4011e-02, -3.4324e-02, -1.3322e-02, -2.1464e-02, -4.0352e-03,\n",
       "                      -1.2164e-02,  2.7720e-02, -3.3178e-02, -7.1842e-03,  6.4074e-03,\n",
       "                      -2.0824e-02,  4.4215e-03, -8.8720e-03,  2.3018e-02,  3.4911e-03,\n",
       "                       1.5735e-02, -3.3762e-04, -7.2907e-03, -2.4222e-02, -3.0504e-02,\n",
       "                       2.5356e-02,  9.3445e-03, -1.8790e-03,  1.5545e-02, -3.9667e-03,\n",
       "                       1.0832e-02,  2.0850e-02,  3.4342e-06, -2.1929e-02, -1.3123e-02,\n",
       "                      -3.0037e-02,  2.8646e-02,  3.2322e-02, -3.0591e-02,  2.7133e-02,\n",
       "                      -3.0284e-02,  4.2356e-03,  2.6344e-02,  1.8651e-02,  2.1225e-02,\n",
       "                       1.7974e-02, -2.7575e-02, -1.2951e-02, -3.0767e-02, -5.4493e-04,\n",
       "                       1.6226e-02,  2.1605e-02, -4.4936e-03,  2.5548e-02, -2.7169e-02,\n",
       "                       1.0133e-02, -2.8255e-02, -1.0436e-02,  3.3283e-02,  1.7320e-02,\n",
       "                       1.2188e-02, -1.4566e-02,  2.1796e-02,  2.8663e-02, -3.1025e-02,\n",
       "                      -4.8840e-03,  2.4010e-03,  3.4309e-03,  1.9177e-02, -2.1166e-03,\n",
       "                       2.8359e-02,  3.3484e-02, -8.0671e-03,  2.3664e-02, -1.8602e-02,\n",
       "                      -2.6753e-02, -5.5428e-03, -1.2423e-02,  3.0966e-02,  2.9423e-02,\n",
       "                      -7.5251e-03, -3.3736e-02,  8.5288e-03, -2.6199e-02, -9.1605e-03,\n",
       "                      -3.2978e-02, -1.2990e-02, -5.1768e-03,  2.6298e-02,  2.7866e-02,\n",
       "                      -1.5164e-02,  2.4567e-02, -2.8058e-02,  2.3604e-02,  3.5045e-03,\n",
       "                       1.1104e-03,  2.7313e-02,  2.0180e-02,  1.0331e-02,  2.1241e-02,\n",
       "                      -1.7881e-02, -3.2451e-02,  1.2902e-02,  6.2510e-03,  2.3011e-02,\n",
       "                      -1.9646e-02, -7.8755e-03, -1.7232e-02,  8.0300e-03,  2.3879e-02,\n",
       "                      -1.1809e-03,  1.3060e-02,  2.1655e-02, -6.7666e-03, -1.5503e-03,\n",
       "                      -6.7409e-03,  1.2246e-02, -1.7326e-02, -1.7469e-02, -3.3479e-02,\n",
       "                      -2.4867e-02,  8.4483e-03, -2.7753e-02,  6.3996e-03,  1.5058e-02,\n",
       "                       2.6776e-02,  1.5046e-02,  5.3799e-03,  1.1850e-02, -1.9007e-02,\n",
       "                      -2.9370e-02,  1.6061e-02,  9.8289e-03,  1.0766e-02, -2.0468e-02,\n",
       "                       2.0360e-02,  1.6163e-02, -7.8247e-03, -1.6831e-02, -2.9452e-02,\n",
       "                       2.7811e-02, -3.0837e-02,  1.4336e-02,  3.4320e-02, -1.9337e-02,\n",
       "                      -2.9629e-02,  2.2660e-03,  2.7372e-02,  2.6338e-02, -3.4107e-02,\n",
       "                      -1.2954e-02, -1.8345e-02, -2.2961e-02,  2.8038e-02,  3.2583e-02,\n",
       "                       2.2306e-02, -5.1717e-03,  2.0114e-02,  1.3352e-04,  6.1422e-03,\n",
       "                       3.4467e-02, -3.0547e-02,  1.5158e-02,  1.4627e-02,  2.7317e-02,\n",
       "                       9.2770e-03, -3.5501e-02,  3.4678e-02, -9.6903e-03,  1.1499e-02,\n",
       "                      -2.6292e-03,  2.1774e-02, -3.0537e-02, -3.1629e-02,  1.5674e-02,\n",
       "                      -3.4489e-02, -8.8685e-03,  1.4489e-02, -4.8181e-03,  1.3336e-02,\n",
       "                      -2.6995e-02, -1.6605e-02,  1.5885e-02,  2.1318e-02,  3.5527e-02,\n",
       "                      -4.9433e-03,  1.8448e-02, -3.5946e-03, -2.0951e-02,  2.2843e-02,\n",
       "                      -3.3247e-02, -5.1300e-03,  2.0105e-02,  2.3558e-02, -2.5341e-02,\n",
       "                      -1.2296e-02,  2.7466e-02, -3.4891e-02, -1.0009e-02,  8.6363e-03,\n",
       "                      -3.4922e-02,  1.5998e-02,  2.3388e-02, -1.0787e-02,  2.2689e-02,\n",
       "                       2.8494e-02, -1.2856e-02, -6.2783e-03,  3.2312e-02, -2.8056e-02,\n",
       "                      -3.4414e-02, -1.7075e-02, -2.7961e-03,  3.1849e-02,  1.2199e-03,\n",
       "                       1.0329e-02, -2.8372e-03,  1.1517e-02,  3.1904e-02,  4.8623e-03,\n",
       "                       1.3601e-03,  2.3019e-02,  4.0578e-03,  2.4579e-02,  1.6353e-02,\n",
       "                      -1.3852e-02,  7.9153e-03,  2.6045e-03,  1.3407e-02, -3.0359e-02,\n",
       "                      -1.9580e-02, -9.2251e-03,  1.6336e-02,  3.2288e-02, -2.2155e-02,\n",
       "                       2.1732e-02, -3.0416e-03,  1.3416e-02,  2.1010e-02,  3.1320e-02,\n",
       "                      -2.0577e-02, -7.8924e-03], device='cuda:0')),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0443, -0.0248,  0.0297,  ..., -0.0272, -0.0359, -0.0128],\n",
       "                      [-0.0042, -0.0006,  0.0436,  ...,  0.0137, -0.0170,  0.0390],\n",
       "                      [ 0.0536, -0.0129,  0.0020,  ...,  0.0050,  0.0196, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0214, -0.0300, -0.0166,  ..., -0.0260, -0.0272, -0.0310],\n",
       "                      [ 0.0029, -0.0095, -0.0282,  ..., -0.0050, -0.0396, -0.0224],\n",
       "                      [-0.0445, -0.0117, -0.0145,  ...,  0.0050, -0.0153,  0.0202]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 1.1225e-02, -3.6441e-02, -1.4674e-02, -2.1062e-02,  3.4445e-02,\n",
       "                       1.8247e-02, -3.4010e-02, -2.5949e-02, -2.0471e-02,  2.1984e-02,\n",
       "                      -6.3969e-04,  2.0958e-02,  3.1265e-02,  1.2575e-02,  3.4651e-02,\n",
       "                       1.5293e-02, -4.6169e-03,  4.1469e-02, -8.0294e-04, -1.3523e-02,\n",
       "                       1.1706e-02, -2.3842e-02, -3.5453e-02, -3.9722e-02, -3.2166e-02,\n",
       "                      -2.0801e-02, -1.9788e-02, -1.8396e-02, -1.7832e-02, -2.4426e-02,\n",
       "                      -3.1077e-02, -1.4068e-02, -8.9885e-03,  6.0960e-03, -2.3554e-02,\n",
       "                      -1.5678e-02, -7.6330e-03,  3.3713e-02,  1.9173e-02,  4.2264e-03,\n",
       "                      -8.4815e-03,  6.1931e-03,  1.5621e-02,  3.8897e-02,  5.1484e-03,\n",
       "                      -1.1160e-02, -4.4638e-02, -3.8770e-02, -6.7550e-04, -4.1301e-02,\n",
       "                       2.9739e-02, -1.0117e-02,  1.9496e-02, -4.4552e-02, -1.5759e-02,\n",
       "                       2.8529e-02,  3.5381e-02,  2.3088e-02, -3.5492e-02,  3.1827e-02,\n",
       "                      -1.9629e-02,  9.6237e-03, -3.5522e-02,  5.8232e-04,  4.7574e-03,\n",
       "                      -2.5091e-02, -3.1863e-02,  1.8722e-02,  1.5887e-02, -1.7451e-02,\n",
       "                      -4.3331e-02, -4.3759e-02, -2.7437e-03, -4.6613e-03,  4.2021e-02,\n",
       "                       1.7833e-02, -2.9243e-02,  4.1349e-02, -2.8117e-02,  7.8301e-04,\n",
       "                      -4.5487e-03,  3.2583e-02, -2.7474e-02, -1.9171e-02,  3.5231e-02,\n",
       "                      -8.7027e-03,  1.6038e-02,  1.9195e-02,  2.5934e-04,  8.6069e-03,\n",
       "                       2.8641e-02, -3.9928e-02,  1.5628e-02,  2.7767e-03, -1.4534e-02,\n",
       "                      -1.7806e-02,  2.0118e-02,  2.7729e-02,  3.8799e-03, -1.0070e-02,\n",
       "                      -4.2894e-02,  2.4088e-02,  1.7706e-02, -3.1792e-02, -2.4035e-03,\n",
       "                      -2.7836e-03, -4.1981e-02, -2.2868e-02, -2.1297e-02,  3.3291e-02,\n",
       "                       1.8435e-02,  1.9177e-02, -3.2110e-02,  9.9773e-03,  3.9309e-02,\n",
       "                       3.7129e-03,  2.7913e-02, -2.6671e-02,  3.6544e-02,  1.1872e-03,\n",
       "                       2.8682e-02, -8.9059e-03, -1.5745e-02,  1.2618e-02,  2.3594e-02,\n",
       "                       3.6920e-02,  2.8470e-02, -1.0938e-03, -3.3801e-02,  2.8150e-02,\n",
       "                      -3.9227e-02, -4.2432e-02,  1.7097e-03, -5.7666e-03, -3.3316e-02,\n",
       "                       1.7981e-02,  2.0310e-02,  2.5972e-02,  1.7562e-02,  2.7122e-02,\n",
       "                      -7.7486e-05, -2.7917e-02, -3.1056e-04,  1.6827e-02, -9.0001e-03,\n",
       "                       3.0493e-02,  2.7194e-02, -1.7442e-02,  4.3915e-02,  2.6439e-02,\n",
       "                      -3.4963e-02, -1.8560e-02, -3.7266e-02,  3.8674e-02,  3.4258e-02,\n",
       "                      -3.0207e-02,  3.8942e-03,  2.1100e-02,  4.2814e-02,  3.7092e-02,\n",
       "                       1.1993e-02,  3.6112e-02, -3.5452e-02, -4.1321e-02,  3.6538e-02,\n",
       "                      -4.4992e-02,  1.0076e-02,  6.2280e-03,  3.2348e-02, -1.3264e-02,\n",
       "                      -3.1494e-02,  3.2620e-02,  2.7504e-02,  4.0308e-02, -1.8785e-02,\n",
       "                      -1.4030e-02, -1.3549e-02,  3.5034e-02, -2.0244e-02, -1.7198e-02,\n",
       "                       3.5618e-02,  3.5451e-02, -3.2747e-03,  8.8661e-03, -1.0413e-02,\n",
       "                       1.7174e-02,  3.7443e-02, -4.2107e-02, -3.3750e-02, -3.5810e-02,\n",
       "                       2.3716e-02,  4.2141e-03, -4.2039e-02,  3.5366e-02, -1.2747e-02,\n",
       "                      -3.1488e-02,  1.7930e-02,  6.1992e-03, -7.2810e-03,  2.1909e-03,\n",
       "                       2.7146e-02, -4.2581e-02,  1.1121e-02, -1.2733e-02, -3.3751e-02,\n",
       "                      -1.7930e-02, -2.6515e-02,  7.9229e-03,  1.7779e-02, -2.8639e-02,\n",
       "                      -3.8916e-02,  1.9440e-02, -1.1794e-02,  2.5195e-02, -2.1434e-02,\n",
       "                       2.0698e-03, -1.4144e-02, -2.2787e-02, -9.6176e-03,  2.5995e-03,\n",
       "                       1.1277e-03,  1.0715e-02, -1.5575e-02,  4.1774e-03,  5.9499e-03,\n",
       "                       2.1469e-02,  5.5073e-03,  4.4319e-02, -1.1562e-02,  2.6881e-02,\n",
       "                       1.1215e-02,  2.4704e-02, -1.8619e-02, -7.0976e-03, -1.0792e-02,\n",
       "                      -9.8965e-03, -2.8909e-02, -3.6893e-02, -1.2875e-03, -4.3321e-02,\n",
       "                       1.5694e-03, -3.8011e-02, -2.9653e-02,  3.6686e-02, -3.1398e-03,\n",
       "                       1.4986e-02, -3.1818e-02, -2.9330e-02,  2.4234e-02,  8.8027e-03,\n",
       "                       3.0425e-02,  1.0989e-02, -2.8990e-02, -4.1007e-02, -3.8485e-02,\n",
       "                       5.6059e-04, -2.9457e-02, -2.8768e-02,  7.5030e-04,  2.0782e-02,\n",
       "                      -8.2316e-03,  4.3061e-04, -3.7616e-02,  1.5007e-02, -1.8985e-02,\n",
       "                      -3.4909e-03,  1.7360e-02, -3.5958e-02,  8.6043e-03, -1.3773e-02,\n",
       "                      -8.9544e-03,  1.6583e-02,  2.2343e-02, -2.7942e-02, -3.1846e-02,\n",
       "                       1.1119e-02, -4.4029e-02,  2.0249e-02,  3.4067e-02,  9.4848e-03,\n",
       "                      -3.6607e-02,  3.6900e-02, -3.0610e-02,  7.4636e-03, -9.5776e-03,\n",
       "                       4.3897e-02, -1.3997e-02, -9.2613e-03,  9.6864e-03, -3.5436e-02,\n",
       "                       1.6871e-03, -3.1422e-02,  1.9528e-02, -2.8154e-02,  1.6752e-02,\n",
       "                       4.1697e-02,  3.6229e-02, -2.0557e-02, -4.1871e-02,  3.1152e-02,\n",
       "                      -3.3603e-02, -3.5123e-02, -2.8127e-02,  1.9419e-02, -8.6891e-03,\n",
       "                       2.7246e-02, -4.1269e-02,  1.5931e-03,  4.2642e-02,  2.8694e-02,\n",
       "                       2.1480e-02, -3.1439e-02,  3.8891e-02,  3.4334e-02,  2.9879e-02,\n",
       "                       3.4671e-02,  2.5129e-02, -2.7766e-02, -1.2082e-02,  3.0440e-02,\n",
       "                       2.4460e-02,  4.3217e-02,  6.8688e-03, -1.9463e-02, -1.5093e-03,\n",
       "                      -3.7665e-02, -3.7091e-02, -2.8284e-02,  3.8064e-03,  1.5421e-02,\n",
       "                       1.6562e-03, -4.4449e-02, -3.5817e-02,  4.3053e-02,  3.3829e-02,\n",
       "                       2.9650e-02, -4.1749e-02, -2.3761e-02, -3.0183e-02,  1.6294e-02,\n",
       "                      -8.4731e-03, -3.0327e-02,  4.2506e-02,  1.5966e-02, -2.8643e-02,\n",
       "                       1.3244e-02,  1.1366e-02, -1.6171e-02,  2.9352e-02, -3.2299e-02,\n",
       "                       4.7933e-03, -1.1248e-02, -2.4387e-02,  2.5159e-02, -1.6173e-02,\n",
       "                      -4.4178e-02,  4.3404e-02, -1.2246e-02, -2.0222e-02, -1.1028e-02,\n",
       "                       3.9088e-02, -7.8339e-03, -6.0104e-03,  1.0891e-02, -3.8631e-02,\n",
       "                       2.5916e-02,  1.5879e-02, -1.3512e-03, -4.1699e-02, -3.8117e-02,\n",
       "                       6.1765e-03, -1.3027e-02,  1.2235e-02,  3.2078e-02,  3.5861e-02,\n",
       "                      -1.1695e-02,  3.5313e-02, -7.3266e-03, -3.5765e-02, -6.1379e-04,\n",
       "                      -2.4157e-02,  1.9526e-03,  1.4128e-02, -8.4953e-03,  5.4361e-03,\n",
       "                       3.3047e-02, -3.0436e-02,  3.0812e-02,  4.6839e-04, -3.4321e-04,\n",
       "                       9.6926e-03, -2.7934e-02,  5.7742e-04, -5.0279e-03, -1.6790e-02,\n",
       "                       2.2115e-02, -2.0526e-02,  2.5868e-02,  1.6558e-02, -4.2416e-02,\n",
       "                      -1.0396e-02,  2.2782e-02, -1.1424e-02, -1.2469e-02,  1.7930e-02,\n",
       "                       7.7948e-03,  3.1616e-02, -1.0550e-02, -3.4963e-02, -1.5018e-02,\n",
       "                       2.5292e-02, -2.2701e-02,  2.3717e-02, -1.7728e-03, -3.1555e-02,\n",
       "                       5.0311e-03, -2.3558e-03, -1.1310e-02, -3.2014e-02,  2.0673e-02,\n",
       "                      -2.6058e-02, -7.1977e-03,  8.1914e-03,  7.2394e-03, -2.7462e-03,\n",
       "                      -2.6500e-02, -3.7238e-02, -4.1613e-02, -1.2125e-02, -1.7942e-02,\n",
       "                      -4.0981e-02, -3.2471e-02,  3.5040e-02,  3.1622e-02,  4.3517e-02,\n",
       "                      -3.3024e-02, -1.4109e-03, -2.2257e-02,  1.4033e-02, -4.3265e-02,\n",
       "                      -2.7399e-02,  4.7051e-03,  2.6362e-02,  4.0964e-02, -2.6433e-02,\n",
       "                      -1.4796e-02,  3.7763e-02,  3.6663e-02, -3.1281e-02,  5.9450e-04,\n",
       "                      -1.8099e-02,  8.9781e-03,  3.7131e-02, -4.0157e-02,  1.5647e-02,\n",
       "                      -9.5555e-03,  3.5475e-03,  2.5821e-02, -4.3529e-02,  3.8427e-02,\n",
       "                       1.1835e-02, -2.0734e-02, -1.1154e-02,  1.3256e-03,  1.0203e-02,\n",
       "                      -4.1180e-02, -1.7170e-02,  7.8055e-03, -8.1146e-03,  2.4022e-02,\n",
       "                       3.7879e-02, -8.9385e-03,  1.7911e-02, -6.9893e-03, -6.2362e-03,\n",
       "                       7.7800e-03,  3.5352e-02, -1.1900e-02, -1.3908e-03,  8.8925e-03,\n",
       "                       2.5679e-02,  6.9871e-03, -4.2865e-02, -4.2950e-02,  2.2712e-02,\n",
       "                       2.0718e-02, -5.7986e-03,  8.1005e-03,  3.0409e-04,  2.6707e-02,\n",
       "                       4.1996e-02,  3.9695e-02, -5.1703e-03, -3.1204e-02, -7.6067e-03,\n",
       "                      -1.3303e-02,  2.4079e-02,  5.2892e-03, -1.9773e-02,  8.4012e-03,\n",
       "                       1.7196e-02, -1.5361e-03, -4.2790e-04, -3.3890e-03,  2.2394e-02,\n",
       "                       1.8425e-02,  7.9429e-03, -1.6894e-02, -4.1873e-02,  2.0814e-02,\n",
       "                      -3.2599e-02, -1.8555e-02], device='cuda:0')),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0319,  0.0357, -0.0440,  ...,  0.0552,  0.0297,  0.0862],\n",
       "                      [-0.0187,  0.0637,  0.0026,  ...,  0.0164, -0.0077,  0.0311],\n",
       "                      [-0.0517,  0.0476, -0.0746,  ...,  0.0213, -0.0196, -0.0387],\n",
       "                      ...,\n",
       "                      [-0.0602,  0.0168,  0.0614,  ..., -0.0011,  0.0179, -0.0354],\n",
       "                      [ 0.0585,  0.0443,  0.0032,  ...,  0.0181,  0.0168,  0.0229],\n",
       "                      [-0.0121, -0.0191, -0.0621,  ...,  0.0241,  0.0122,  0.0146]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0300, -0.0218, -0.0206,  0.0455, -0.0640, -0.0439, -0.0180, -0.0116,\n",
       "                      -0.0220, -0.0291], device='cuda:0'))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 가중치 - model.state_dict() 로 확인가능\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'checkpoints/NNtest.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN().to(device)\n",
    "model.load_state_dict(torch.load('checkpoints/NNtest.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98fb658fc8779e6136f671765f304aa90b381a69e0e3fde0fbe10c53a5640cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
